import os
from torch.utils.data import DataLoader,TensorDataset
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from tqdm import tqdm
import csv
from model import Transformer  # å‡è®¾ Transformer å®šä¹‰åœ¨ model.py æ–‡ä»¶ä¸­
from data_processing import load_and_process_data  # å‡è®¾æ•°æ®å¤„ç†åœ¨ data_processing.py ä¸­
from transformers import MarianTokenizer

# è¶…å‚æ•°è®¾ç½®ï¼ˆä¸è®ºæ–‡ä¸€è‡´ï¼‰
SRC_VOCAB_SIZE = 58101  # æ ¹æ®è®ºæ–‡ï¼ŒWMT 2014 æ•°æ®é›†ä½¿ç”¨äº† 37000 çš„æºè¯­è¨€è¯æ±‡è¡¨
TGT_VOCAB_SIZE = 58101  # ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°
D_MODEL = 512  # æ¨¡å‹ç»´åº¦ 512
NUM_HEADS = 8  # å¤šå¤´æ³¨æ„åŠ›å¤´æ•°
NUM_LAYERS = 6  # ç¼–ç å™¨å’Œè§£ç å™¨çš„å±‚æ•°
D_FF = 2048  # å‰é¦ˆç½‘ç»œçš„éšè—å±‚ç»´åº¦
MAX_SEQ_LENGTH = 128  # æœ€å¤§åºåˆ—é•¿åº¦  128
BATCH_SIZE = 64  # æ¯æ‰¹æ¬¡å¤§å°
EPOCHS = 1   # è®­ç»ƒå‘¨æœŸæ•°  10
LEARNING_RATE = 0.0001  # å­¦ä¹ ç‡
WARMUP_STEPS = 4000  # è®ºæ–‡ä¸­çš„ warmup æ­¥æ•°

# å®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨
class TransformerLRScheduler:
    def __init__(self, d_model, warmup_steps):
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        self.step_num = 0  # è®­ç»ƒæ­¥æ•°

    def step(self, optimizer):
        self.step_num += 1
        lr = (self.d_model ** -0.5) * min(self.step_num ** -0.5, self.step_num * (self.warmup_steps ** -1.5))
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
        return lr

# ç»˜åˆ¶æŸå¤±æ›²çº¿
def plot_loss(steps, losses, save_path):
    plt.figure(figsize=(10, 6))
    plt.plot(steps, losses, linewidth=1.5, color='blue')
    plt.xlabel('Batch', fontsize=14)
    plt.ylabel('Loss', fontsize=14)
    plt.title('Loss every 200 batches', fontsize=16)
    plt.grid(True, which="both", linestyle='--', linewidth=0.5)
    plt.savefig(save_path)
    plt.close()

# è®­ç»ƒå‡½æ•°
def train():
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # åŠ è½½é¢„å¤„ç†åçš„ç´¢å¼•æ•°æ®
    en_idx = torch.load('./en_processed_indexes.pt')
    de_idx = torch.load('./de_processed_indexes.pt')

    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    dataset = TensorDataset(en_idx, de_idx)  # å°†è‹±æ–‡å’Œå¾·æ–‡æ•°æ®é…å¯¹
    dataloader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        shuffle=True  # æ¯ä¸ªepochå‰æ‰“ä¹±æ•°æ®
    )

    # åˆ›å»ºæ¨¡å‹
    model = Transformer(
        src_vocab_size=SRC_VOCAB_SIZE,
        tgt_vocab_size=TGT_VOCAB_SIZE,
        d_model=D_MODEL,
        num_heads=NUM_HEADS,
        num_layers=NUM_LAYERS,
        d_ff=D_FF,
        max_seq_length=MAX_SEQ_LENGTH,
        dropout=0.1
    ).to(device)

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss(ignore_index=1)  # å¿½ç•¥å¡«å……ç¬¦
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)
    scheduler = TransformerLRScheduler(d_model=D_MODEL, warmup_steps=WARMUP_STEPS)

    # ç”¨äºè®°å½•æŸå¤±
    losses = []
    steps = []
    step_count = 0

    # ç”¨äºæ¯200ä¸ªbatchè®°å½•æ—¥å¿—
    batch_log_data = []
    epoch_log_data = []

    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0
        batch_iterator = tqdm(dataloader, desc=f"Epoch {epoch + 1}", leave=False)

        for src_batch, tgt_batch in batch_iterator:
            src_batch = src_batch.to(device)
            tgt_batch = tgt_batch.to(device)

            optimizer.zero_grad()

            output = model(src_batch, tgt_batch[:, :-1])
            
            # è®¡ç®—æŸå¤±
            loss = criterion(output.view(-1, TGT_VOCAB_SIZE), tgt_batch[:, 1:].contiguous().view(-1))
            loss.backward()
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.8)
            optimizer.step()
            current_lr = scheduler.step(optimizer)
            train_loss += loss.item()

            batch_iterator.set_postfix(loss=loss.item(), lr=current_lr)
            step_count += 1
            if step_count % 200 == 0:
                losses.append(loss.item())
                steps.append(step_count)
                plot_loss(steps, losses, 'transformer_loss.png')

                # è®°å½•æ¯200ä¸ªbatchçš„è®­ç»ƒæ—¥å¿—
                batch_log_data.append({
                    'Epoch': epoch + 1,
                    'Batch': step_count,
                    'Loss': loss.item(),
                    'Learning Rate': current_lr
                })

        epoch_loss = train_loss / len(dataloader)
        print(f'ğŸš€ Epoch {epoch + 1}/{EPOCHS} Loss: {epoch_loss:.4f}')

        # æ¯ä¸ªepochç»“æŸåä¿å­˜æ¨¡å‹
        save_path = f"transformer_epoch_{epoch + 1}_weights.pth"
        torch.save(model.state_dict(), save_path)
        print(f"ğŸš€ æƒé‡å·²ä¿å­˜: {save_path}")

        # è®°å½•æ¯ä¸ªepochçš„è®­ç»ƒæ—¥å¿—
        epoch_log_data.append({
            'Epoch': epoch + 1,
            'Epoch Loss': epoch_loss,
            'Final Learning Rate': current_lr
        })

        # ä¿å­˜ epoch æ—¥å¿—
        epoch_csv_save_path = 'epoch_training_logs.csv'
        os.makedirs(os.path.dirname(epoch_csv_save_path), exist_ok=True)
        with open(epoch_csv_save_path, 'w', newline='') as csvfile:
            fieldnames = ['Epoch', 'Epoch Loss', 'Final Learning Rate']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            for log in epoch_log_data:
                writer.writerow(log)

    # ä¿å­˜æ¯200ä¸ªbatchçš„è®­ç»ƒæ—¥å¿—
    batch_csv_save_path = 'batch_training_logs.csv'
    os.makedirs(os.path.dirname(batch_csv_save_path), exist_ok=True)
    with open(batch_csv_save_path, 'w', newline='') as csvfile:
        fieldnames = ['Epoch', 'Batch', 'Loss', 'Learning Rate']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for log in batch_log_data:
            writer.writerow(log)
    print(f"æ¯200ä¸ªbatchè®­ç»ƒæ—¥å¿—å·²ä¿å­˜åˆ° {batch_csv_save_path}")


# å¯åŠ¨è®­ç»ƒ
if __name__ == "__main__":
    train()
