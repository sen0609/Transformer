{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "## Target:Realize Transformer,try to run de-en dataset on it with settings same to the paper and compare the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realize encoder part(the encoder part is composed by N(6 in the paper) encoder blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realize Multihead attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,num_heads):  \n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        assert d_model % num_heads ==0,\"d_model must be divisible by num_head\"\n",
    "\n",
    "        self.d_model = d_model           # Dimension of the model\n",
    "        self.num_heads = num_heads       # Number of heads\n",
    "        self.d_k = d_model // num_heads  # Dimension of each head\n",
    "\n",
    "        # Define the Liner transfomation layers(no bias requiered)\n",
    "        self.W_q = nn.Linear(d_model,d_model)\n",
    "        self.W_k = nn.Linear(d_model,d_model)\n",
    "        self.W_v = nn.Linear(d_model,d_model)\n",
    "        self.W_o = nn.Linear(d_model,d_model)\n",
    "\n",
    "\n",
    "\n",
    "    def ScaledDotProductAttention(self,Q,K,V,mask = None):\n",
    "        \"\"\"\n",
    "        Compute scaled dot product attention.\n",
    "        The three input matrix are Q(Query),K(Key),V(Value).\n",
    "        The shape of the input matrix above is (batch_size,num_heads,seq_length,d_k).\n",
    "        The shape of the output matrix is same.\n",
    "        Mask operation is optional.\n",
    "        \"\"\"\n",
    "        # Compute attention scores(Q·K^T/sqrt(d_k))\n",
    "        attn_scores = nn.matmul(Q,K.transpose(-2,-1)) / math.sqrt(self.d_k)  #the shape of it is (batch_size,num_heads,seq_length,seq_length) \n",
    "\n",
    "        # Mask\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0 , -1e9)\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = nn.softmax(attn_scores,dim = -1)\n",
    "\n",
    "        # Compute the weighted sum of value vector\n",
    "        output = nn.matmul(attn_weights,V)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def split_heads(self,x):\n",
    "        \"\"\"\n",
    "        Split the input tensor into many heads\n",
    "        The shape of the input is (batch_size,seq_length,d_model)\n",
    "        The shape of the output is (batch_size,num_heads,seq_length,d_k)\n",
    "        \"\"\"\n",
    "        bacth_size,seq_length,d_model = x.size()\n",
    "        return x.view(bacth_size,seq_length,self.num_heads,self.d_k).transpose(1,2)\n",
    "    \n",
    "    def concat_heads(self,x):\n",
    "        \"\"\"\n",
    "        Merge the output of multiple heads back into the original shape\n",
    "        The shape of the input is (batch_size,num_heads,seq_length,d_k)\n",
    "        The shape of the output is (batch_size,seq_length,d_model)\n",
    "        \"\"\"\n",
    "        batch_size,seq_length,d_k = x.size()\n",
    "        return x.transpose(1,2).contiguous().view(batch_size,seq_length,self.d_model)\n",
    "    \n",
    "    def forward(self,Q,K,V,mask = None):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        The shape of the input is (batch_size,seq_length,d_model)\n",
    "        The shape of the output is (batch_size,seq_length,d_model)\n",
    "        \"\"\"\n",
    "\n",
    "        # Linear trasformation and split the multiheads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        # Compute attention\n",
    "        attn_output = self.ScaledDotProductAttention(Q,K,V,mask)\n",
    "\n",
    "        # merge the multiheads and do linear transformation\n",
    "        output = self.W_o(self.concat_heads(attn_output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realize the Position-wise Feed-Forward Network\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self,d_model,d_ff):\n",
    "        super(PositionWiseFeedForward,self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model,d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff,d_model)\n",
    "        self.relu = nn.RELU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Positioinal ENcoding\n",
    "class PositonalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,max_seq_length):\n",
    "        super(PositonalEncoding,self).__init__()\n",
    "        pe = nn.zeros(max_seq_length,d_model)\n",
    "        position = nn.arange(0,max_seq_length,dtype=nn.float).unsqueeze(1)\n",
    "        div_term = nn.exp(nn.arange(0,d_model,2).float()*-(math.log(10000.0)/d_model))\n",
    "\n",
    "        pe[:,0::2] = nn.sin(position*div_term)\n",
    "        pe[:,1::2] = nn.cos(position*div_term)\n",
    "        self.register_buffer('pe',pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # add the positinal encoding into the input \n",
    "        return x + self.pe[:,:x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulid the Encoder Block\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,d_model,num_heads,d_ff,dropout):\n",
    "        super(EncoderBlock,self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model,num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model,d_ff)\n",
    "        self.norm1 = nn.layer_norm(d_model)\n",
    "        self.norm2 = nn.layer_norm(d_model)\n",
    "        self.dropout = nn.dropout(dropout)\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        # Multihead attention\n",
    "        attn_output = self.self_attn(x,x,x,mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feed Forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x+self.dropout(ff_output))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realize Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Block\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,d_model,num_heads,d_ff,dropout):\n",
    "        super(DecoderBlock,self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model,num_heads)\n",
    "        self.cross_atnn = MultiHeadAttention(d_model,num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model,d_ff)\n",
    "        self.norm1 = nn.layer_norm(d_model)\n",
    "        self.norm2 = nn.layer_norm(d_model)\n",
    "        self.norm3 = nn.layer_norm(d_model)\n",
    "        self.dropout = nn.dropout(dropout)\n",
    "\n",
    "    def forward(self,x,enc_output,src_mask,tgt_mask):\n",
    "        attn_output = self.self_attn(x,x,x,tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        attn_output = self.cross_atnn(x,enc_output,enc_output,src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build complete Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,src_vocab_size,tgt_vocab_size,d_model,num_heads,num_layers,d_ff,max_seq_length,dropout):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.encoder_embedding = nn.embedding(src_vocab_size,d_model)\n",
    "        self.decoder_embedding = nn.embedding(tgt_vocab_size,d_model)\n",
    "        self.positional_encoding = PositonalEncoding(d_model,max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.MoudleList([EncoderBlock(d_model,num_heads,d_ff,dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.MoudleList([DecoderBlock(d_model,num_heads,d_ff,dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model,tgt_vocab_size)\n",
    "        self.dropout = nn.dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        # 源掩码：屏蔽填充符（假设填充符索引为0）\n",
    "        # 形状：(batch_size, 1, 1, seq_length)\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "   \n",
    "        # 目标掩码：屏蔽填充符和未来信息\n",
    "        # 形状：(batch_size, 1, seq_length, 1)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        # 生成上三角矩阵掩码，防止解码时看到未来信息\n",
    "        nopeak_mask = (1 - nn.triu(nn.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask  # 合并填充掩码和未来信息掩码\n",
    "        return src_mask, tgt_mask\n",
    "    \n",
    "    def forward(self,src,tgt):\n",
    "        # Generate mask\n",
    "        src_mask,tgt_mask = self.generate_mask(src,tgt)\n",
    "\n",
    "        # Encoder part\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output,src_mask)\n",
    "\n",
    "        # Decoder part\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output,enc_output,src_mask,tgt_mask)\n",
    "\n",
    "        # Final output \n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nndl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
